<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="Drv9VlgLtwHUs1lwpjfMLYrq60TahBaTiMjalRV9r8s"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Kernel methods | Alvaro Gonzalez Hernandez </title> <meta name="author" content="Alvaro Gonzalez Hernandez"> <meta name="description" content="Study group on Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, number-theory, mathematics"> <meta property="og:site_name" content="Alvaro Gonzalez Hernandez"> <meta property="og:type" content="website"> <meta property="og:title" content="Alvaro Gonzalez Hernandez | Kernel methods"> <meta property="og:url" content="https://AlvaroGohe.github.io/projects/kernel_methods/"> <meta property="og:description" content="Study group on Machine Learning"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Kernel methods"> <meta name="twitter:description" content="Study group on Machine Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Kummer_Logo.png?9311d66ffc6ddc5d29a8ad101819c60e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alvarogohe.github.io/projects/kernel_methods/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alvaro</span> Gonzalez Hernandez </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About me </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/code/">Code </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/travel/">Travel </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"> <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Kernel methods</h1> <p class="post-description">Study group on Machine Learning</p> </header> <article> <p>I gave this talk on the 27th of May of 2025 for the <a href="https://www.marctruter.com/reading-groups" rel="external nofollow noopener" target="_blank">study group on Machine Learning</a> organised by <a href="https://www.marctruter.com/home" rel="external nofollow noopener" target="_blank">Marc Truter</a> and <a href="https://warwick.ac.uk/fac/sci/maths/people/staff/lambley/" rel="external nofollow noopener" target="_blank">Hefin Lambley</a>. It had a theoretical component, where I explained the theory behind kernel methods, and a practical component, where I showed how to implement them using the <code class="language-plaintext highlighter-rouge">sklearn</code> library.</p> <div style="padding-bottom: 100px; padding-top: 50px;"> # Theory Kernel methods are a powerful set of techniques in machine learning that are used for building non-linear prediction models. In previous weeks of the study group, we saw how to construct linear models that allow us to find trends in data. However, as we know, most phenomena that machine learning excels at studying, from image recognition to natural language processing, are inherently non-linear. Kernels provide a way to solve non-linear problems by transforming them into equivalent problems that can be solved efficiently using linear methods. ## Let's start with an example A very illustrative example is the following: Suppose that we want to build a model that allows us to classify data into two categories, and we have the following training data: <div align="center"> <img src="/assets/img/kernel_pics/data.png" alt="Some points in the plane" height="350"> </div> <div style="height: 50px;"></div> We would like to be able to find a curve in the coordinate plane that splits the plane into two regions, one for each category. In this case, it is quite clear that using a line to separate the data is not going to work well, but it seems like this data could be approximated well by a closed shape, for example, a circle. Now, here is the trick on how to classify the data. Instead of trying to fit a linear model on the data $(x_1,x_2)\in\mathbb{R}^2$, we generate points in $\mathbb{R}^5$ of the form $(x_1,x_2,x_1^2,x_1x_2,x_2^2)$ and try to use a hyperplane to separate the data. Indeed, just by considering $\{x_1^2,x_2^2\}$, we can easily see that the data can be easily separated with a line: <div align="center"> <img src="/assets/img/kernel_pics/classification_lines.png" alt="The points can be separated by a line" height="300"> </div> <div style="max-width: 100%; height: auto;"></div> Now, this line corresponds to a conic in the original space, which separates our original data: <div align="center"> <img src="/assets/img/kernel_pics/classification_circles.png" alt="The points can be separated by a conic" height="350"> </div> <div style="max-width: 100%; height: auto;"></div> This idea of mapping the data into a higher-dimensional space is the essence of kernel methods. In the exercise session, we will see how to implement them in practice using the `sklearn` library. But let's first explain the theory behind the kernel approach. ## Theoretical background As in the past, let $(x_i,y_i)\in\mathcal{X}\times\mathcal{Y}$ be a set of training data of size $n$, where $\mathcal{X}\subseteq\mathbb{R}^d$ is the input space and $\mathcal{Y}$ is the output space. Our goal is to learn a function $f\colon\mathcal{X}\to\mathcal{Y}$ that approximates the relationship between the inputs and outputs. For today's talk, we will focus on the case where $\mathcal{Y}=\{-1,1\}$, which is the case of binary classification. As $\mathcal{Y}$ is discrete, what we do instead is that we learn a function $f_\theta\colon\mathcal{X}\to\mathbb{R}$ and then, we set $f(x)=\text{sign}(f_\theta(x))$. This function $f_\theta$ is called the **prediction function**, and we will assume that it is a linear function on some parameters $\{\theta_1,\dots,\theta_m\}$. Assume we also have a **loss function** $\ell\colon\mathcal{Y}\times\mathbb{R}\to\mathbb{R}$ that measures how well the prediction function $f_\theta$ fits the data. For example, we can consider the square loss $\ell(y,f_\theta(x))=(y-f_\theta(x))^2$. Let us now choose a function $\varphi\colon\mathcal{X}\rightarrow\mathbb{R}^m$ that maps the input space $\mathcal{X}$ to a different space $\mathbb{R}^m$. This is what we will call the **feature map**. Going back to our example, there, we defined the feature map to be $$\begin{align*} \mathcal{X}&amp;\longrightarrow \mathbb{R}^3\\ (x_1,x_2)&amp;\longmapsto (x_1^2, x_2^2, 1) \end{align*}$$ and the function we are trying to learn is $f_\theta(x_1,x_2)=\theta_1 x_1^2 + \theta_2 x_2^2+\theta_3$, for some parameters $\theta_1,\theta_2,\theta_3\in\mathbb{R}$. If we denote by $(x_i,y_i)$ our data, we can write $f_\theta(x_i)$ in terms of the usual inner product as $f_\theta(x_i)=\langle\theta, \varphi(x_i)\rangle$. We would like to find the parameter $\theta=(\theta_1,\dots,\theta_m)\in\mathbb{R}^m$ that minimises what is known as the **empirical risk**: $$ \begin{align*} \frac{1}{n}\sum_{i=1}^n \ell(y_i,\langle\theta, \varphi(x_i)\rangle)+\frac{\lambda}{2}\lVert \theta\rVert^2 &amp;&amp;(1) \end{align*} $$ The first term of this expression represents how close the $f_\theta(x_i)$ are to the correct values in our training data, whereas $\frac{\lambda}{2}\lVert \theta\rVert^2$ is a regularization term that prevents overfitting by penalizing large values of the parameter $\theta$. We can use linear regression to find the parameter $\theta$ that minimises this expression. ## Introducing kernels From the discussion above, we saw that we can reduce non-linear problems to linear ones by using a feature map $\varphi$. In practice, there are two issues with this approach: - We often do not know how to choose a good feature map $\varphi$ and we may need to try several ones before finding one that works well. - Solving the optimisation problem above can be computationally expensive, especially if the input data has very large dimension. This is something common in many problems, particularly, when we have very sparsely populated data. To model mathematically how it is to work with very high dimensional data, we can use Hilbert spaces. Recall that a **Hilbert space** is a vector space (possibly of infinite dimension) with an inner product space that is complete with respect to the norm induced by the inner product. Assume that the feature map $\varphi$ now takes values in $\mathcal{H}$ rather than $\mathbb{R}^{m}$. One would imagine that solving the optimisation problem given by $(1)$ in this setting is now even more difficult, as we may be working in a space with infinite dimension. But there is a theorem that guarantees that the difficulty of the problem only depends on the number of elements in our training data, not on the size of the input space $\mathcal{X}$: <hr> **Representer Theorem (for supervised learning).** For $\lambda&gt;0$, the infimum of the empirical risk $$ \inf_{\theta\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^n \ell(y_i,\langle\theta, \varphi(x_i)\rangle)+\frac{\lambda}{2}\lVert \theta\rVert^2$$ can be obtained by restricting to a vector $\theta$ of the form: $$\theta=\sum_{i=1}^n \alpha_i \varphi(x_i)$$ where $\alpha=(\alpha_1,\dots,\alpha_n)\in\mathbb{R}^n$. <hr> <div style="height: 30px;"></div> Now, let us define the **kernel function** $k$ induced by the feature map $\varphi$ to be $$\begin{align*}k:\mathcal{X}\times\mathcal{X}&amp;\longrightarrow\mathbb{R}\\ (x,x')&amp;\longmapsto\langle \varphi(x),\varphi(x')\rangle \end{align*}$$ Let $K \in \mathbb{R}^{n \times n}$ be the **kernel matrix** whose entries are given by $K_{ij}=k(x_i,x_j)$. Then, if we have $\theta=\sum_{i=1}^n \alpha_i \varphi(x_i)$, $$\langle\theta, \varphi(x_j)\rangle=\sum_{i=1}^n \alpha_i k(x_i,x_j)=(K\alpha)_j$$ and $$\lVert\theta\rVert^2=\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\langle\varphi(x_i),\varphi(x_j)\rangle=\alpha^\top K \alpha $$ so that we can rewrite the optimisation problem as $$\inf_{\theta\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^n \ell(y_i,\langle\theta, \varphi(x_i)\rangle)+\frac{\lambda}{2}\lVert \theta\rVert^2=\inf_{\alpha\in\mathbb{R}^n}\frac{1}{n}\sum_{i=1}^n \ell(y_i,(K\alpha)_i)+\frac{\lambda}{2}\alpha^\top K \alpha$$ This what is known as the **kernel trick**: instead of working with the feature map $\varphi$, we can work directly with the kernel matrix $K$. This allows us to work with high-dimensional data without having to explicitly compute the feature map $\varphi$. As a matter of fact, we can forget about the feature map $\varphi$ altogether and just work with the kernel function $k$, for the following reason: We say that a function $k \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a **positive-definite kernel** if, for any finite set of points $x_1,\dots,x_n\in\mathcal{X}$, the kernel matrix $K$ restricted to those points is symmetric positive semi-definite. Then, we have this theorem: <hr> **Theorem (Aronszajn, 1950)** The function $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is a positive-definite kernel if and only if there exists a Hilbert space $\mathcal{H}$ and a function $\varphi:\mathcal{X}\rightarrow\mathcal{H}$ such that for all $x,x'\in\mathcal{X}$, $k(x,x')=\langle \varphi(x),\varphi(x')\rangle$. <hr> <div style="height: 30px;"></div> Therefore, the existence of a kernel function $k$ is equivalent to the existence of a feature map $\varphi$ in some Hilbert space. For any positive-definite kernel $k$, the space that we build from the kernel is called the **reproducing kernel Hilbert space (RKHS)**. There are many other reasons why working with kernels is useful, including the fact that there are very efficient algorithms to compute kernels and solve their optimisation problem. ## Examples of kernels There are many different kernels that can be used in practice. Some of the most common ones are: <ul> <li> **Linear kernel**: $k(x,x')=x^\top x'$, $\forall x,x'\in\mathcal{X}\subseteq\mathbb{R}^d$. Here, the kernel trick can be useful when the input data have huge dimension $d$, but is quite sparse, such as in text processing. </li> <li> **Polynomial kernel**: $k(x,x')=(1+x^\top x')^s$, $\forall x,x'\in\mathcal{X}\subseteq\mathbb{R}^d$. The image of the feature map is the set of all polynomials on $d$ variables of degree at most $s$.</li> <li> **Homogeneous polynomial kernel**: $k(x,x')=(x^\top x')^s$, $\forall x,x'\in\mathcal{X}\subseteq\mathbb{R}^d$. The image of the feature map is the set of degree $s$ homogeneous polynomials on $\mathbb{R}^d$.</li> <li> **Radial basis function (RBF) kernel**: $k(x,x')=\exp\left(-\gamma\lVert x-x'\rVert^2\right)$, $\forall x,x'\in\mathcal{X}\subseteq\mathbb{R}^d$. This kernel is also known as the **Gaussian kernel**, and it is particularly useful in many applications, such as image processing and natural language processing. The parameter $\gamma&gt;0$ controls the boundary of the decision region, in the sense that as $\gamma$ grows, the boundary becomes more complicated and can fit the data better, but it also increases the risk of overfitting.</li> <li> **Translation invariant kernels**: $k(x,x')=q(x-x')$, $\forall x,x'\in[0,1]$. The idea behind this class of kernels is that the space of square-integrable functions on $[0,1]$ is a Hilbert space with the inner product given by $\langle f,g\rangle=\int_0^1 f(x)g(x)\,dx$. An orthonormal basis of $L_2([0, 1])$ is given by the functions $\sin(2\pi m x)$ and $\cos(2\pi n x)$ for $m,n\in\mathbb{N}$, and we can study many aspects of $q(x)$ from the perspective of Fourier analysis. These kernels are particularly useful in time series analysis, where we can use them to study periodic phenomena (e.g., weather patterns, seasonal financial trends...) </li> </ul> We will see how to work with some of these kernels in practice in the exercise session. <div style="height: 50px;"></div> # Practice In this practical session, we are going to be coding the problem that we discussed at the start of the talk: how to use kernels to solve non-linear classification problems. The techniques used to divide a set of observations in the decision space using hyperplanes are usually referred to as **Support Vector Machines** (abbreviated as SVMs). The reason why they are called that way is because what they do is to maximise the distance of the hyperplane to the **support vectors**, which are the data points that are closest to the decision surface. ## Learning how to use SVMs in Python We start by importing the libraries that we will need for this exercise: ```python # Imports import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_moons from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score ``` Now, we are going to generate the data that we will use for the exercise. We will use the `make_moons` function from `sklearn.datasets`, which generates a two-dimensional dataset with a non-linear decision boundary. ```python # Generate synthetic 2D data (nonlinear) X, y = make_moons(n_samples=200, noise=0.2, random_state=13) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # We use the train_test_split function to split the dataset into training and testing sets. ``` The function `SVC` from `sklearn.svm` implements the Support Vector Machine algorithm. It takes as arguments the kernel to be used, the regularisation parameter $C$, and the degree of the polynomial kernel if it is used. The regularisation parameter $C$ is related to the $\lambda$ that we explained in the talk and controls the trade-off between maximizing the margin, which is the distance to the support vectors and minimising the classification error. A small value of $C$ will result in a larger margin but may misclassify some points, while a large value of $C$ will result in a smaller margin but will classify all points correctly. ```python # Here are some examples of SVM kernels that can be used for classification tasks: kernels = { "Linear": SVC(kernel="linear", C=1), "Polynomial (degree=7)": SVC(kernel="poly", degree=7, C=1), "RBF (γ=1)": SVC(kernel="rbf", gamma=1, C=1), } ``` Recall that we use `fit` to train the model and `predict` to make predictions. We can compute the accuracy of the model by comparing the predicted labels with the true labels using the `accuracy_score` function from `sklearn.metrics`. ```python ## We see that the accuracy of the linear kernel is 85% linkernel = SVC(kernel="linear", C=1) linkernel.fit(X_train, y_train) y_pred = linkernel.predict(X_test) acc = accuracy_score(y_test, y_pred) print(acc) ``` Let's now plot the decision boundary of the SVM model. We will create a `plot_decision_boundary` function to visualize the decision boundary. ```python # Plot decision boundary def plot_decision_boundary(ker, X, y, title): h = 0.02 x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5 y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = ker.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.figure(figsize=(6, 5)) plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.coolwarm) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors="k") plt.title(title) plt.xlabel("x1") plt.ylabel("x2") plt.tight_layout() plt.show() ``` Now, we can compare how the three types of kernels compare in terms of accuracy and decision boundary. ```python for name, ker in kernels.items(): ker.fit(X_train, y_train) y_pred = ker.predict(X_test) acc = accuracy_score(y_test, y_pred) plot_decision_boundary(ker, X, y, f"{name} Kernel — Accuracy: {acc:.2f}") ``` <div style="display: flex; flex-direction: row; gap: 10px; justify-content: center;"> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-1.png" alt="Linear kernel" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-2.png" alt="Polynomial kernel" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-3.png" alt="RBF kernel" style="max-width: 100%; height: auto;"> </div> </div> <div style="height: 50px;"></div> ## Now it's your turn! For this exercise session, we are going to compute the decision boundary of a Support Vector Machine using different kernels. We are going to generate the data from a picture. In this case, our image is the Warwick logo, and we are going to sample the points according to whether they are black or white pixels in the image. Feel free to use any image you like! &gt; **Note**: To work with images, we'll use the `scikit-image` package, which we import using the name `skimage`. &gt; &gt; As in talk 1, you can install this using the `pip` package manager (for most users, just type `pip install scikit-image` in your terminal, just like we did at the start of the reading group). ```python from skimage import io, color from skimage.filters import gaussian # Load image img = io.imread("warwick.jpg") # should be a black-and-white silhouette plt.imshow(img) plt.axis("off") plt.title("Original Image") plt.show() # Convert to grayscale gray = color.rgb2gray(img) # Apply Gaussian blur so the edges are not too sharp gray = gaussian(gray, sigma=1) # Threshold to get a binary mask threshold = 0.3 maskb = gray &lt; threshold # black = foreground maskw = gray &gt; 1 - threshold # white = foreground # Get coordinates of black pixels coordsb = np.column_stack(np.where(maskb)) coordsw = np.column_stack(np.where(maskw)) # Rescale both coordsb and coordsw to the same [-1, 1] square using global min/max all_coords = np.vstack([coordsb, coordsw]).astype(float) min0, max0 = all_coords[:, 0].min(), all_coords[:, 0].max() min1, max1 = all_coords[:, 1].min(), all_coords[:, 1].max() coordsb = coordsb.astype(float) coordsw = coordsw.astype(float) coordsb[:, 0] = 2 * (coordsb[:, 0] - min0) / (max0 - min0) - 1 coordsb[:, 1] = 2 * (coordsb[:, 1] - min1) / (max1 - min1) - 1 coordsw[:, 0] = 2 * (coordsw[:, 0] - min0) / (max0 - min0) - 1 coordsw[:, 1] = 2 * (coordsw[:, 1] - min1) / (max1 - min1) - 1 # The orientation is questionable, so we flip the axes dum = coordsb.copy() coordsb[:, 0] = dum[:, 1] coordsb[:, 1] = -1 * dum[:, 0] dum = coordsw.copy() coordsw[:, 0] = dum[:, 1] coordsw[:, 1] = -1 * dum[:, 0] # Subsample randomly from both sets of coordinates n_points = 800 # This is the number of points to sample from each set idxb = np.random.choice(len(coordsb), size=n_points, replace=False) idxw = np.random.choice(len(coordsw), size=n_points, replace=False) XB = coordsb[idxb] XW = coordsw[idxw] # Visualise plt.figure(figsize=(6, 6)) plt.scatter(XB[:, 0], XB[:, 1], s=2, color="black") # note flipped axes plt.scatter(XW[:, 0], XW[:, 1], s=2, color="white") # note flipped axes plt.title("Random samples from the image") plt.axis("equal") plt.gca().set_facecolor("lightgray") # or any color you prefer, e.g. "#f0f0f0" plt.show() ``` <div style="display: flex; flex-direction: row; gap: 10px; justify-content: center;"> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-4.png" alt="Original image" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-5.png" alt="Random samples from the image" style="max-width: 100%; height: auto;"> </div> </div> <div style="height: 50px;"></div> ## Exercises ### Exercise 1 Now that we have generated the data, let's try to train the models. We have two arrays, `XB` and `XW` that contain the coordinates of the black and white pixels, respectively. Your first task is to produce training and testing sets from these arrays. You can use the `train_test_split` function from `sklearn.model_selection` to do this. It takes as arguments two arrays $X$ with the data and $y$ with the labels of $X$; the test size, and a random state (that is used for reproducibility). To compute $X$ and $y$, you may find useful to use the following functions: <ul> <li> `np.vstack` and `np.hstack` can be used to stack arrays vertically and horizontally, respectively. </li> <li> `np.zeros` and `np.one` can be used to produce arrays of length $n$ with only zeros or ones, respectively. </li> </ul> ```python # Create training data from XB (label 0) and XW (label 1) X = __ print(X) y = __ print(y) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=__, random_state=__) ``` <div style="height: 20px;"></div> ### Exercise 2 Let's now compare how different kernels perform on this dataset. More specifically, we will compare how does the polynomial kernel perform with different degrees. First, we will create a dictionary containing all polynomial kernels of degrees from 1 to 9. More specifically, what I want is a dictionary whose keys are "Polynomial of degree d" and the values are the SVM models with the polynomial kernel of degree $d$, with $d$ ranging from 1 to 9. ```python # Create a dictionary of polynomial kernels with different degrees. polykernels = {"__": SVC(kernel="__", degree=__) for d in __} print(polykernels) ``` <div style="height: 20px;"></div> ### Exercise 3 Let's do a table with the accuracy of each model. You can use the `pandas` library to create a data frame with the results. The data frame should have two columns: "Kernel" and "Accuracy". The "Kernel" column should contain the name of the kernel, and the "Accuracy" column should contain the accuracy of the model. ```python # Create a data frame to store the name and accuracy of each kernel. import pandas as pd results = __ for name, ker in polykernels.items(): __.fit(__, __) # Fit the model y_pred = __.predict(__) # Predict the labels for the test set acc = accuracy_score(__, __) # Compute the accuracy results.append( {"Kernel": __, "Accuracy": __} ) # Add starting rows of the data frame df_results = pd.DataFrame(__) print(__) ``` <div style="height: 20px;"></div> ### Exercise 4 Finally, we will plot the decision boundaries of the models in the dictionary. You can use the `plot_decision_boundary` function that we defined earlier to do this. ```python # Make a plot with the decision boundaries of each of the polynomial kernels that includes the name and the accuracy. for name, ker in polykernels.items(): ker.fit(__, __) y_pred = ker.predict(__) acc = accuracy_score(__, __) plot_decision_boundary(ker, X, y, f"{__} — Accuracy: {__:.2f}") ``` <div style="height: 20px;"></div> Finally, a geometric question to reflect on: why do you think the kernels with even degrees perform better than the kernels with odd degrees? What is the reason behind this? You can also experiment with the different kernels and see how they perform on the dataset. You can also try to change the regularisation parameter $C$ and see how it affects the decision boundary. <div style="height: 50px;"></div> <hl> ## Solutions ### Solution to Exercise 1 ```python # Create training data from XB (label 0) and XW (label 1) X = np.vstack([XB, XW]) print(X) y = np.hstack([np.zeros(len(XB)), np.ones(len(XW))]) print(y) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) ``` ``` text Output: [[-0.33641618 0.31278891] [-0.60924855 -0.24191063] [ 0.0982659 0.51617874] ... [ 0.97456647 -0.14021572] [ 0.90520231 -0.13097072] [ 0.49364162 0.15562404]] [0. 0. 0. ... 1. 1. 1.] ``` <div style="height: 20px;"></div> ### Solution to Exercise 2 ```python # Create a dictionary of polynomial kernels with different degrees polykernels = { f"Polynomial of degree {d}": SVC(kernel="poly", degree=d) for d in range(1, 10) } print(polykernels) ``` ``` text Output: {'Polynomial of degree 1': SVC(degree=1, kernel='poly'), 'Polynomial of degree 2': SVC(degree=2, kernel='poly'), 'Polynomial of degree 3': SVC(kernel='poly'), 'Polynomial of degree 4': SVC(degree=4, kernel='poly'), 'Polynomial of degree 5': SVC(degree=5, kernel='poly'), 'Polynomial of degree 6': SVC(degree=6, kernel='poly'), 'Polynomial of degree 7': SVC(degree=7, kernel='poly'), 'Polynomial of degree 8': SVC(degree=8, kernel='poly'), 'Polynomial of degree 9': SVC(degree=9, kernel='poly')} ``` <div style="height: 20px;"></div> ### Solution to Exercise 3 ```python import pandas as pd results = [] for name, ker in polykernels.items(): ker.fit(X_train, y_train) y_pred = ker.predict(X_test) acc = accuracy_score(y_test, y_pred) results.append({"Kernel": name, "Accuracy": acc}) df_results = pd.DataFrame(results) print(df_results) ``` ``` text Output: Kernel Accuracy 0 Polynomial of degree 1 0.606250 1 Polynomial of degree 2 0.809375 2 Polynomial of degree 3 0.678125 3 Polynomial of degree 4 0.831250 4 Polynomial of degree 5 0.603125 5 Polynomial of degree 6 0.834375 6 Polynomial of degree 7 0.593750 7 Polynomial of degree 8 0.815625 8 Polynomial of degree 9 0.571875 ``` <div style="height: 20px;"></div> ### Solution to Exercise 4 ```python # Make a plot with the decision boundaries of each of the polynomial kernels that includes the name and the accuracy. for name, ker in polykernels.items(): ker.fit(X_train, y_train) y_pred = ker.predict(X_test) acc = accuracy_score(y_test, y_pred) plot_decision_boundary(ker, X, y, f"{name} — Accuracy: {acc:.2f}") ``` <div style="display: flex; flex-direction: row; gap: 10px; justify-content: center;"> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-6.png" alt="Polynomial of degree 1" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-7.png" alt="Polynomial of degree 2" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-8.png" alt="Polynomial of degree 3" style="max-width: 100%; height: auto;"> </div> </div> <div style="display: flex; flex-direction: row; gap: 10px; justify-content: center; margin-top: 10px;"> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-9.png" alt="Polynomial of degree 4" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-10.png" alt="Polynomial of degree 5" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-11.png" alt="Polynomial of degree 6" style="max-width: 100%; height: auto;"> </div> </div> <div style="display: flex; flex-direction: row; gap: 10px; justify-content: center; margin-top: 10px;"> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-12.png" alt="Polynomial of degree 7" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-13.png" alt="Polynomial of degree 8" style="max-width: 100%; height: auto;"> </div> <div style="flex: 1; text-align: center;"> <img src="/assets/img/kernel_pics/image-14.png" alt="Polynomial of degree 9" style="max-width: 100%; height: auto;"> </div> </div> </hl> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alvaro Gonzalez Hernandez. Website powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-WYQ5X6ZG58"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-WYQ5X6ZG58");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about-me",title:"About me",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-research",title:"Research",description:"",section:"Navigation",handler:()=>{window.location.href="/research/"}},{id:"nav-talks",title:"Talks",description:"",section:"Navigation",handler:()=>{window.location.href="/talks/"}},{id:"nav-code",title:"Code",description:"",section:"Navigation",handler:()=>{window.location.href="/code/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-travel",title:"Travel",description:"",section:"Navigation",handler:()=>{window.location.href="/travel/"}},{id:"post-some-pictures-of-me-giving-talks",title:"Some pictures of me giving talks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/pictures_talks/"}},{id:"news-i-created-this-website",title:"I created this website.",description:"",section:"News"},{id:"news-i-am-still-working-on-this-website-sweat-smile",title:'I am still working on this website! <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">',description:"",section:"News"},{id:"news-the-website-is-ready",title:"The website is ready!",description:"",section:"News"},{id:"news-the-travel-tab-has-been-added-airplane",title:'The Travel tab has been added! <img class="emoji" title=":airplane:" alt=":airplane:" src="https://github.githubassets.com/images/icons/emoji/unicode/2708.png" height="20" width="20">',description:"",section:"News"},{id:"news-my-new-preprint-is-out",title:"My new preprint is out!",description:"",section:"News"},{id:"news-i-just-returned-from-a-conference-in-durham-european-castle",title:'I just returned from a conference in Durham <img class="emoji" title=":european_castle:" alt=":european_castle:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f0.png" height="20" width="20">',description:"",section:"News"},{id:"news-v2-of-my-paper-explicit-desingularisation-of-kummer-surfaces-in-characteristic-two-via-specialisation-is-out",title:"v2 of my paper Explicit desingularisation of Kummer surfaces in characteristic two via...",description:"",section:"News"},{id:"news-i-gave-a-talk-in-the-phd-open-day-for-fourth-year-warwick-students",title:"I gave a talk in the PhD open day for fourth-year Warwick students....",description:"",section:"News"},{id:"news-i-gave-a-talk-about-affine-geometric-invariant-theory-in-the-study-group-on-git",title:"I gave a talk about Affine Geometric Invariant Theory in the study group...",description:"",section:"News"},{id:"news-merry-christmas",title:"Merry Christmas!",description:"",section:"News",handler:()=>{window.location.href="/news/announcement10/"}},{id:"news-i-travelled-to-bilbao-to-attend-the-vii-congreso-de-j\xf3venes-investigadores-de-la-rsme",title:"I travelled to Bilbao to attend the VII Congreso de J\xf3venes Investigadores de...",description:"",section:"News",handler:()=>{window.location.href="/news/announcement11/"}},{id:"news-i-gave-a-talk-about-two-theorems-of-odaka-at-the-study-group-on-k-stability",title:"I gave a talk about two theorems of Odaka at the study group...",description:"",section:"News"},{id:"news-i-gave-a-talk-at-the-junior-number-theory-seminar-of-the-university-of-warwick",title:"I gave a talk at the Junior Number Theory seminar of the University...",description:"",section:"News"},{id:"news-i-gave-a-talk-at-the-number-theory-seminar-of-the-university-of-manchester",title:"I gave a talk at the Number Theory seminar of the University of...",description:"",section:"News"},{id:"news-i-gave-a-talk-at-the-warwick-glasgow-conference",title:"I gave a talk at the Warwick-Glasgow Conference.",description:"",section:"News"},{id:"projects-an-introduction-to-the-hasse-principle-through-examples",title:"An introduction to the Hasse principle through examples",description:"Warwick's Postgraduate Seminar",section:"Projects",handler:()=>{window.location.href="/projects/an_introduction_to_the_hasse_principle_through_examples/"}},{id:"projects-hyperelliptic-curves-and-their-jacobians",title:"Hyperelliptic curves and their jacobians",description:"Study group on isogeny-based cryptography",section:"Projects",handler:()=>{window.location.href="/projects/hyperelliptic_curves_and_their_jacobians/"}},{id:"projects-elliptic-curves-over-discrete-valuation-rings",title:"Elliptic curves over discrete valuation rings",description:"Study group on Mazur's torsion theorem",section:"Projects",handler:()=>{window.location.href="/projects/elliptic_curves_over_dvrs/"}},{id:"projects-intersection-theory-on-surfaces",title:"Intersection theory on surfaces",description:"Study group on Hasse-Weil zeta functions",section:"Projects",handler:()=>{window.location.href="/projects/intersection_theory_on_surfaces/"}},{id:"projects-the-eisenstein-ideal",title:"The Eisenstein ideal",description:"Study group on Mazur's torsion theorem",section:"Projects",handler:()=>{window.location.href="/projects/the_eisenstein_ideal/"}},{id:"projects-curves-surfaces-and-singularities-in-positive-characteristic",title:"Curves, surfaces and singularities in positive characteristic",description:"Series of talks",section:"Projects",handler:()=>{window.location.href="/projects/curves,_surfaces_and_singularities_in_characteristic_p/"}},{id:"projects-an-arithmetic-expedition-into-k3-surfaces",title:"An arithmetic expedition into K3 surfaces",description:"Warwick Junior Number Theory Seminar",section:"Projects",handler:()=>{window.location.href="/projects/an_arithmetic_expedition_into_K3_surfaces/"}},{id:"projects-a-game-based-learning-intervention-for-support-classes-in-mathematics",title:"A game-based learning intervention for support classes in mathematics",description:"Warwick Education Conference 2023",section:"Projects",handler:()=>{window.location.href="/projects/a_game-based_learning_intervention_for_support_classes_in_mathematics/"}},{id:"projects-kummer-surfaces-bridging-the-gap-between-geometry-and-number-theory",title:"Kummer surfaces, bridging the gap between geometry and number theory",description:"Queer in Number Theory and Geometry",section:"Projects",handler:()=>{window.location.href="/projects/kummer_surface_bridging_the_gap/"}},{id:"projects-crazy-for-two-genus-two-curves-in-characteristic-two-via-kummer-surfaces",title:"Crazy for two, genus two curves in characteristic two via Kummer surfaces",description:"Young Researchers in Algebraic Number Theory (Y-RANT)",section:"Projects",handler:()=>{window.location.href="/projects/crazy_for_two/"}},{id:"projects-explicit-theory-of-kummer-surfaces-in-characteristic-two",title:"Explicit theory of Kummer surfaces in characteristic two",description:"Linfoot Number Theory Seminar",section:"Projects",handler:()=>{window.location.href="/projects/explicit_theory_of_Kummer_surfaces_in_characteristic_two_copy/"}},{id:"projects-rigidity-and-the-construction-of-the-dual-abelian-variety",title:"Rigidity and the construction of the dual abelian variety",description:"Study group on abelian varieties",section:"Projects",handler:()=>{window.location.href="/projects/rigidity_and_the_construction_of_the_dual_abelian_variety/"}},{id:"projects-gamma-n-structures",title:"Gamma(n) structures",description:"Study group on moduli problems of elliptic curves",section:"Projects",handler:()=>{window.location.href="/projects/gamma_structures/"}},{id:"projects-affine-geometric-invariant-theory",title:"Affine Geometric Invariant Theory",description:"Study group on Geometric Invariant Theory",section:"Projects",handler:()=>{window.location.href="/projects/affine_git/"}},{id:"projects-can-dogs-do-maths",title:"Can dogs do maths?",description:"Video for Tom Rocks Maths",section:"Projects",handler:()=>{window.location.href="/projects/can_dogs_do_math/"}},{id:"projects-local-normal-forms-of-non-commutative-potentials",title:"Local normal forms of non-commutative potentials",description:"RGAS School on Singularities",section:"Projects",handler:()=>{window.location.href="/projects/local_normal_forms_of_non-commutative_potentials/"}},{id:"projects-explicit-theory-of-kummer-surfaces-in-characteristic-two",title:"Explicit theory of Kummer surfaces in characteristic two",description:"Hodge Club",section:"Projects",handler:()=>{window.location.href="/projects/explicit_theory_of_kummer_surfaces_in_characteristic_two/"}},{id:"projects-how-to-desingularise-kummer-surfaces",title:"How to desingularise Kummer surfaces",description:"Junior Geometry Seminar of KCL/UCL",section:"Projects",handler:()=>{window.location.href="/projects/how_to_desingularise_Kummer_surfaces/"}},{id:"projects-masses-of-central-leaves-and-automorphisms-of-abelian-varieties",title:"Masses of central leaves and automorphisms of abelian varieties",description:"Arizona Winter School",section:"Projects",handler:()=>{window.location.href="/projects/masses_of_central_leaves_and_automorphisms_of_abelian_varieties/"}},{id:"projects-explicit-models-of-kummer-surfaces-in-characteristic-two",title:"Explicit models of Kummer surfaces in characteristic two",description:"7th Symposium of the Roman Number Theory Association",section:"Projects",handler:()=>{window.location.href="/projects/explicit_models_of_kummer_surfaces_in_characteristic_two/"}},{id:"projects-some-examples-of-chow-rings-and-applications-to-intersection-theory",title:"Some examples of Chow rings and applications to intersection theory",description:"Study group on intersection theory",section:"Projects",handler:()=>{window.location.href="/projects/some_examples_of_chow_rings/"}},{id:"projects-explicit-construction-of-k3-surfaces-with-everywhere-good-reduction",title:"Explicit construction of K3 surfaces with everywhere good reduction",description:"Oberseminar on Arithmetische Geometrie",section:"Projects",handler:()=>{window.location.href="/projects/explicit_construction_of_K3_surfaces_with_everywhere_good_reduction.png/"}},{id:"projects-k3-surfaces-with-everywhere-good-reduction",title:"K3 surfaces with everywhere good reduction",description:"Young Researchers in Algebraic Number Theory",section:"Projects",handler:()=>{window.location.href="/projects/k3_surfaces_with_everywhere_good_reduction/"}},{id:"projects-the-life-of-a-phd-student-in-warwick",title:"The life of a PhD student in Warwick",description:"PhD open day",section:"Projects",handler:()=>{window.location.href="/projects/the_life_of_a_phd_student_in_warwick/"}},{id:"projects-intersections-of-the-automorphism-and-p-rank-strata-inside-of-m-2",title:"Intersections of the automorphism and p-rank strata inside of (M_2)",description:"VII Congreso de J\xf3venes Investigadores de la RSME",section:"Projects",handler:()=>{window.location.href="/projects/intersections_of_the_automorphism_and_p-rank_strata/"}},{id:"projects-two-theorems-of-odaka",title:"Two theorems of Odaka",description:"Study group on K-stability",section:"Projects",handler:()=>{window.location.href="/projects/odakas_theorems/"}},{id:"projects-intersections-of-the-automorphism-and-p-rank-strata-inside-of-m-2",title:"Intersections of the automorphism and p-rank strata inside of (M_2)",description:"Warwick's Junior Number Theory Seminar",section:"Projects",handler:()=>{window.location.href="/projects/intersections_of_the_automorphism_and_p-rank_strata_copy/"}},{id:"projects-intersections-of-the-automorphism-and-p-rank-strata-inside-of-m-2",title:"Intersections of the automorphism and p-rank strata inside of (M_2)",description:"Manchester's Number Theory Seminar",section:"Projects",handler:()=>{window.location.href="/projects/intersections_of_the_automorphism_and_p-rank_strata_copy2/"}},{id:"projects-ag\xe1rrate-que-vienen-curvas-algebraicas",title:"\xa1Ag\xe1rrate, que vienen curvas (algebraicas)!",description:"XXIII Encuentro de Estudiantes de Matem\xe1ticas",section:"Projects",handler:()=>{window.location.href="/projects/agarrate_que_vienen_curvas_algebraicas/"}},{id:"projects-identificaci\xf3n-de-grupos-planos-de-simetr\xeda-en-mosaicos-nazar\xedes",title:"Identificaci\xf3n de grupos planos de simetr\xeda en mosaicos nazar\xedes",description:"XX Encuentro de Estudiantes de Matem\xe1ticas",section:"Projects",handler:()=>{window.location.href="/projects/identificacion_de_grupos_planos_de_simetria/"}},{id:"projects-k3-surfaces-with-everywhere-good-reduction",title:"K3 surfaces with everywhere good reduction",description:"D\xe9cimas Jornadas de Teor\xeda de N\xfameros",section:"Projects",handler:()=>{window.location.href="/projects/k3_surfaces_with_everywhere_good_reduction_copy/"}},{id:"projects-teor\xeda-de-trenzas-aplicada-al-dise\xf1o-de-pulseras",title:"Teor\xeda de trenzas aplicada al dise\xf1o de pulseras",description:"XXIII Encuentro de Estudiantes de Matem\xe1ticas",section:"Projects",handler:()=>{window.location.href="/projects/teoria_de_trenzas_aplicada_al_dise%C3%B1o_de_pulseras/"}},{id:"projects-kernel-methods",title:"Kernel methods",description:"Study group on Machine Learning",section:"Projects",handler:()=>{window.location.href="/projects/kernel_methods/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6C%76%61%72%6F.%67%6F%6E%7A%61%6C%65%7A-%68%65%72%6E%61%6E%64%65%7A@%77%61%72%77%69%63%6B.%61%63.%75%6B","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0001-4537-807X","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/AlvaroGohe","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/alvaro-gohe","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://warwick.ac.uk/fac/sci/maths/people/staff/gonzalez-hernandez/","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>